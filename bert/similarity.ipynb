{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练句子相似度比较模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googletrans\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/f0/a22d41d3846d1f46a4f20086141e0428ccc9c6d644aacbfd30990cf46886/googletrans-2.4.0.tar.gz\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (from googletrans) (2.7.0)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Building wheel for googletrans (setup.py): started\n",
      "  Building wheel for googletrans (setup.py): finished with status 'done'\n",
      "  Created wheel for googletrans: filename=googletrans-2.4.0-cp36-none-any.whl size=15789 sha256=dcc72d2b75ba9fa4de2de8fbd9cb658629f6a0dd23727622280404df67ec72fe\n",
      "  Stored in directory: C:\\Users\\Raytine\\AppData\\Local\\pip\\Cache\\wheels\\50\\d6\\e7\\a8efd5f2427d5eb258070048718fa56ee5ac57fd6f53505f95\n",
      "Successfully built googletrans\n",
      "Installing collected packages: googletrans\n",
      "Successfully installed googletrans-2.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# 翻译爬虫\n",
    "!pip install googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator(service_urls=[\n",
    "      'translate.google.cn'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "rootdir = \"E:/workspace/py/NLPHomework/Assignment-04/out/\"\n",
    "fr=open(rootdir+'sentences','r',encoding='utf-8')\n",
    "sentences = LineSentence(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from chinese_bert_similarity/bert_sim_model/model.ckpt-6123\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from os import path\n",
    "sys.path.append( './chinese_bert_similarity')\n",
    "from chinese_bert_similarity.bert_sim import BertSim\n",
    "\n",
    "bs = BertSim(gpu_no=0, log_dir='chinese_bert_similarity/log/', bert_sim_dir='chinese_bert_similarity/bert_sim_model/', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en finish\n"
     ]
    }
   ],
   "source": [
    "# 1. 数据集中加入翻译数据作为高相似度的正样本\n",
    "en_data = []\n",
    "with open('news-commentary-v13.zh-en.en','r',encoding='utf-8') as en_file:\n",
    "    for line in en_file.readlines():\n",
    "        if '&' in line:\n",
    "            line = line.replace('\\n','').replace('&nbsp;',' ').replace('&amp;','&').replace('&#45;','-').replace('&#160;',' ') \\\n",
    "                .replace('&hellip;','…').replace('&quot;','\"')\n",
    "        en_data.append(line)\n",
    "print('en finish')\n",
    "zh_data = []\n",
    "with open('news-commentary-v13.zh-en.zh','r',encoding='utf-8') as zh_file:\n",
    "    for line in zh_file.readlines():\n",
    "        zh_data.append(line.replace('\\n',''))\n",
    "sim = [1 for _ in zh_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>zh</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1929 or 1989?</td>\n",
       "      <td>1929年还是1989年?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PARIS – As the economic crisis deepens and wid...</td>\n",
       "      <td>巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At the start of the crisis, many people likene...</td>\n",
       "      <td>一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today, the mood is much grimmer, with referenc...</td>\n",
       "      <td>如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tendency is either excessive restraint (Eu...</td>\n",
       "      <td>目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                                      1929 or 1989?   \n",
       "1  PARIS – As the economic crisis deepens and wid...   \n",
       "2  At the start of the crisis, many people likene...   \n",
       "3  Today, the mood is much grimmer, with referenc...   \n",
       "4  The tendency is either excessive restraint (Eu...   \n",
       "\n",
       "                                                  zh  sim  \n",
       "0                                      1929年还是1989年?    1  \n",
       "1  巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正...    1  \n",
       "2  一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为...    1  \n",
       "3  如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政...    1  \n",
       "4                  目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。    1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# trans_data = []\n",
    "trans_data_continue = []\n",
    "# translations = translator.translate(en_data, dest='zh-CN',src='en')\n",
    "# for translation in translations:\n",
    "#     trans_data.append(translation.text)\n",
    "for sentence in en_data[27942:]:\n",
    "    try:\n",
    "        translation = translator.translate([sentence], dest='zh-CN',src='en')\n",
    "        time.sleep(1)\n",
    "    #     print(translation[0].text)\n",
    "        trans_data_continue.append(translation[0].text)\n",
    "    except:\n",
    "        print(sentence)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14472\n"
     ]
    }
   ],
   "source": [
    "print(len(trans_data_continue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for translation in trans_data_continue:\n",
    "    trans_data.append(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27942\n"
     ]
    }
   ],
   "source": [
    "print(len(trans_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = pd.DataFrame({'translation':trans_data,'zh':zh_data[:27942],'sim':sim[:27942]})\n",
    "sim_data.to_csv('sim_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>zh</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1929年或1989年？</td>\n",
       "      <td>1929年还是1989年?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>巴黎 - 随着经济危机的加深和加宽，世界一直在寻找历史类比来帮助我们理解正在发生的事情。</td>\n",
       "      <td>巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>在危机开始时，很多人把它比做1982年或1973年，这是令人欣慰的，因为这两个日期均指传统的...</td>\n",
       "      <td>一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>今天，心情是非常严峻，与开始的比比皆是，即使有些国家的政府继续表现得好像危机较特殊的更经典的...</td>\n",
       "      <td>如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>目前的趋势是要么过度限制（欧洲）或努力的扩散（美国）。</td>\n",
       "      <td>目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         translation  \\\n",
       "0                                       1929年或1989年？   \n",
       "1       巴黎 - 随着经济危机的加深和加宽，世界一直在寻找历史类比来帮助我们理解正在发生的事情。   \n",
       "2  在危机开始时，很多人把它比做1982年或1973年，这是令人欣慰的，因为这两个日期均指传统的...   \n",
       "3  今天，心情是非常严峻，与开始的比比皆是，即使有些国家的政府继续表现得好像危机较特殊的更经典的...   \n",
       "4                        目前的趋势是要么过度限制（欧洲）或努力的扩散（美国）。   \n",
       "\n",
       "                                                  zh  sim  \n",
       "0                                      1929年还是1989年?    1  \n",
       "1  巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正...    1  \n",
       "2  一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为...    1  \n",
       "3  如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政...    1  \n",
       "4                  目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。    1  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 打乱文本顺序,生成不相似数据\n",
    "import copy\n",
    "trans_data_2 = copy.deepcopy(trans_data)\n",
    "zh_2 = copy.deepcopy(zh_data)\n",
    "\n",
    "import random\n",
    "random.shuffle(trans_data_2)\n",
    "random.shuffle(zh_2)\n",
    "sim_2 = []\n",
    "for idx in range(len(trans_data_2)):\n",
    "    trans = trans_data_2[idx]\n",
    "    zh = zh_2[idx]\n",
    "    sim_2.append(bs.predict(trans,zh))\n",
    "    \n",
    "trans_data += trans_data_2\n",
    "zh_data += zh_2\n",
    "sim += sim_2\n",
    "\n",
    "random.shuffle(trans_data)\n",
    "random.shuffle(zh_data)\n",
    "random.shuffle(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = pd.DataFrame({'translation':trans_data,'zh':zh_data,'sim':sim})\n",
    "sim_data.to_csv('sim_data.csv')\n",
    "data = sim_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理完毕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import re\n",
    "\n",
    "Texts = torch.LongTensor\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, device):\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = tokenizer.pad_token_id\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for i, (row) in tqdm(dataframe.iterrows()):\n",
    "            if len(tokenizer.tokenize(row[\"zh\"])) > 120:\n",
    "                continue\n",
    "            # 移除第一句标点符号\n",
    "            sentence1 = re.sub(r'[。，?]','',row[\"zh\"])\n",
    "            orignal_text = tokenizer.encode(sentence1)\n",
    "#             orignal_text = torch.LongTensor(text)\n",
    "            # 移除第二句标点符号\n",
    "            sentence2 = re.sub(r'[。，?]','',row[\"translation\"])\n",
    "#             print(sentence2)\n",
    "            trans_text = tokenizer.encode(sentence2, add_special_tokens=True)\n",
    "#             print(tokenizer.decode(text[1:]))\n",
    "            text = torch.LongTensor(orignal_text+trans_text)\n",
    "            \n",
    "            tags = torch.FloatTensor(row[[\"sim\"]])\n",
    "            self.X.append(text)\n",
    "            self.Y.append(tags)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Texts, torch.LongTensor]:\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(data, test_size=0.05)\n",
    "print(len(train_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: List[Tuple[Texts, torch.LongTensor]]) \\\n",
    "        -> Tuple[Texts, torch.LongTensor]:\n",
    "    x, y = list(zip(*batch))\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = torch.stack(y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "train_dataset = SentenceDataset(tokenizer, train_df, device)\n",
    "dev_dataset = SentenceDataset(tokenizer, val_df, device)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = RandomSampler(dev_dataset)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . 训练bert下游模型\n",
    "class BertSimilarity(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertSimilarity, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.similarity = nn.Linear(config.hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "            labels=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "        cls_output = outputs[1] # batch, hidden\n",
    "        cls_output = self.similarity(cls_output) # batch, 2\n",
    "#         print(cls_output.shape)\n",
    "        cls_output = torch.sigmoid(cls_output)\n",
    "        criterion = nn.BCELoss()\n",
    "#         print(cls_output.shape,labels.shape)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output, labels)\n",
    "        return loss, cls_output\n",
    "\n",
    "# model = BertModel.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "model = BertSimilarity.from_pretrained('./model').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def train(model, iterator, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        mask = (x != 0).float()\n",
    "#         print(x.shape,y.shape)\n",
    "        loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Train loss {total_loss / len(iterator)}\")\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(iterator):\n",
    "            mask = (x != 0).float()\n",
    "            loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "            total_loss += loss\n",
    "            true += y.cpu().numpy().tolist()\n",
    "            pred += outputs.cpu().numpy().tolist()\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    print(true.shape,pred.shape)\n",
    "    for i, name in enumerate(['non-similarity','similarity']):\n",
    "        print(f\"{name} roc_auc {roc_auc_score(true[:, i], pred[:, i])}\")\n",
    "    print(f\"Evaluate loss {total_loss / len(iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "EPOCH_NUM = 1\n",
    "# triangular learning rate, linearly grows untill half of first epoch, then linearly decays \n",
    "warmup_steps = int(0.5 * len(train_iterator))\n",
    "total_steps = len(train_iterator) * EPOCH_NUM - warmup_steps\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCH_NUM):\n",
    "    print('=' * 50, f\"EPOCH {i}\", '=' * 50)\n",
    "    train(model, train_iterator, optimizer, scheduler)\n",
    "    evaluate(model, dev_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "output_dir='./sim_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
