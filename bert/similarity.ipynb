{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, AdamW, BertPreTrainedModel, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "395849\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>但是，许多公司并没有把冗余现金用于新投资以扩大产能、占领新市场——自全球金融危机爆发以来它们...</td>\n",
       "      <td>但有担心，之前印度的政治家们决定采取行动的东西比日常苦难更可怕必须发生。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>当释放到一个人的循环系统，血红蛋白在这些高氧化态最终自毁，周围组织损伤的分子。</td>\n",
       "      <td>当血红蛋白被释放到人的循环系统中时，高价铁形式最终会自我毁灭，伤害周围组织的分子。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>是的，世界需要解决全球变暖问题（主要是通过高投资，绿色的研究和开发，并通过促进廉价的开采，污...</td>\n",
       "      <td>是的，世界需要解决全球变暖问题（主要通过加大对绿色能源研究和发展的投资，促进廉价和低污染页岩...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>以色列前安全高官��积极参与了由新美国战略重心（Center for a New Ameri...</td>\n",
       "      <td>有很好的理由：这两个世界上人口最多的民主国家之间的友好关系可以塑造世界的未来。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>有见及此，自1990年以来的每一份联合国发展报告都表明如果某地的公共政策能把民生放在首位，则...</td>\n",
       "      <td>是否在神的创造惊叹或只是试着去了解为什么事情会是这样的 - 但要认识到穆斯林国家有多少可以被...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          sentence1  \\\n",
       "0           0  但是，许多公司并没有把冗余现金用于新投资以扩大产能、占领新市场——自全球金融危机爆发以来它们...   \n",
       "1           1            当释放到一个人的循环系统，血红蛋白在这些高氧化态最终自毁，周围组织损伤的分子。   \n",
       "2           2  是的，世界需要解决全球变暖问题（主要是通过高投资，绿色的研究和开发，并通过促进廉价的开采，污...   \n",
       "3           3  以色列前安全高官��积极参与了由新美国战略重心（Center for a New Ameri...   \n",
       "4           4  有见及此，自1990年以来的每一份联合国发展报告都表明如果某地的公共政策能把民生放在首位，则...   \n",
       "\n",
       "                                           sentence2  similarity  \n",
       "0               但有担心，之前印度的政治家们决定采取行动的东西比日常苦难更可怕必须发生。           0  \n",
       "1          当血红蛋白被释放到人的循环系统中时，高价铁形式最终会自我毁灭，伤害周围组织的分子。           1  \n",
       "2  是的，世界需要解决全球变暖问题（主要通过加大对绿色能源研究和发展的投资，促进廉价和低污染页岩...           1  \n",
       "3            有很好的理由：这两个世界上人口最多的民主国家之间的友好关系可以塑造世界的未来。           0  \n",
       "4  是否在神的创造惊叹或只是试着去了解为什么事情会是这样的 - 但要认识到穆斯林国家有多少可以被...           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_data = pd.read_csv(\"data.csv\")\n",
    "print(len(sim_data))\n",
    "sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 打乱文本顺序,生成不相似数据\n",
    "data = sim_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "import re\n",
    "\n",
    "Texts = torch.LongTensor\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, device):\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = tokenizer.pad_token_id\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for i, (row) in tqdm(dataframe.iterrows()):\n",
    "            if len(tokenizer.tokenize(row[\"sentence1\"])) > 120:\n",
    "                continue\n",
    "            # 移除第一句标点符号\n",
    "            sentence1 = re.sub(r'[。，?]','',row[\"sentence1\"])\n",
    "            orignal_text = tokenizer.encode(sentence1)\n",
    "#             orignal_text = torch.LongTensor(text)\n",
    "            # 移除第二句标点符号\n",
    "            sentence2 = re.sub(r'[。，?]','',row[\"sentence2\"])\n",
    "#             print(sentence2)\n",
    "            trans_text = tokenizer.encode(sentence2, add_special_tokens=True)\n",
    "#             print(tokenizer.decode(text[1:]))\n",
    "            text = torch.LongTensor(orignal_text+trans_text)\n",
    "#             print(type(row[[\"similarity\"]].tolist()),row[[\"similarity\"]].tolist())\n",
    "            sim_value = int(row[[\"similarity\"]].tolist()[0])\n",
    "            tags = torch.FloatTensor([1-sim_value,sim_value])\n",
    "#             print(tags)\n",
    "#             print(tags)\n",
    "            self.X.append(text)\n",
    "            self.Y.append(tags)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Texts, torch.LongTensor]:\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('./model')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(data, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376056\n",
      "19793\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "376056it [09:06, 688.14it/s]\n",
      "19793it [00:28, 697.14it/s]\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch: List[Tuple[Texts, torch.LongTensor]]) \\\n",
    "        -> Tuple[Texts, torch.LongTensor]:\n",
    "    x, y = list(zip(*batch))\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = torch.stack(y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "train_dataset = SentenceDataset(tokenizer, train_df, device)\n",
    "dev_dataset = SentenceDataset(tokenizer, val_df, device)\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = RandomSampler(dev_dataset)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# . 训练bert下游模型\n",
    "class BertSimilarity(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertSimilarity, self).__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.similarity = nn.Linear(config.hidden_size, 2)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "            labels=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "#         print(outputs.shape)\n",
    "#         cls_output = outputs[0][:,0,:] # batch, hidden\n",
    "        cls_output = outputs[1] # output[:, 0]->Linear->Tanh ->(batch, hidden)\n",
    "        cls_output = self.similarity(cls_output) # batch, 2\n",
    "#         print(cls_output.shape)\n",
    "        cls_output = torch.sigmoid(cls_output)\n",
    "        criterion = nn.BCELoss()\n",
    "#         print(cls_output.shape,labels.shape)\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output, labels)\n",
    "        return loss, cls_output\n",
    "\n",
    "# model = BertModel.from_pretrained('hfl/chinese-bert-wwm-ext')\n",
    "model = BertSimilarity.from_pretrained('./model').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def train(model, iterator, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        mask = (x != 0).float()\n",
    "#         print(x.shape,y.shape)\n",
    "        loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Train loss {total_loss / len(iterator)}\")\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(iterator):\n",
    "            mask = (x != 0).float()\n",
    "            loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "            total_loss += loss\n",
    "            true += y.cpu().numpy().tolist()\n",
    "            pred += outputs.cpu().numpy().tolist()\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "#     print(true.shape,pred.shape)\n",
    "    for i, name in enumerate(['non-similarity','similarity']):\n",
    "        print(f\"{name} roc_auc {roc_auc_score(true[:, i], pred[:, i])}\")\n",
    "    print(f\"Evaluate loss {total_loss / len(iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "EPOCH_NUM = 4\n",
    "# triangular learning rate, linearly grows untill half of first epoch, then linearly decays \n",
    "warmup_steps = int(0.5 * len(train_iterator))\n",
    "total_steps = len(train_iterator) * EPOCH_NUM - warmup_steps\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== EPOCH 0 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93914/93914 [4:59:59<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.19179787686077668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4944/4944 [03:16<00:00, 25.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-similarity roc_auc 0.9768990039385351\n",
      "similarity roc_auc 0.9768934935092332\n",
      "Evaluate loss 0.17497685551643372\n",
      "================================================== EPOCH 1 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93914/93914 [4:59:18<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.16408025716438154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4944/4944 [03:16<00:00, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-similarity roc_auc 0.9808903759968274\n",
      "similarity roc_auc 0.9808952689674788\n",
      "Evaluate loss 0.16057398915290833\n",
      "================================================== EPOCH 2 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 93914/93914 [4:59:23<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.13382706855373264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4944/4944 [03:15<00:00, 25.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-similarity roc_auc 0.984459670963238\n",
      "similarity roc_auc 0.984459702095607\n",
      "Evaluate loss 0.14583845436573029\n",
      "================================================== EPOCH 3 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉             | 86452/93914 [4:36:26<23:41,  5.25it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH_NUM):\n",
    "    print('=' * 50, f\"EPOCH {i}\", '=' * 50)\n",
    "    train(model, train_iterator, optimizer, scheduler)\n",
    "    evaluate(model, dev_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sim_model\\\\vocab.txt',\n",
       " './sim_model\\\\special_tokens_map.json',\n",
       " './sim_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型\n",
    "output_dir='./sim_model'\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(s1,s2):\n",
    "    # 移除第一句标点符号\n",
    "    sentence1 = re.sub(r'[。，?]','',s1)\n",
    "    orignal_text = tokenizer.encode(sentence1)\n",
    "#             orignal_text = torch.LongTensor(text)\n",
    "    # 移除第二句标点符号\n",
    "    sentence2 = re.sub(r'[。，?]','',s2)\n",
    "#             print(sentence2)\n",
    "    trans_text = tokenizer.encode(sentence2, add_special_tokens=True)\n",
    "#             print(tokenizer.decode(text[1:]))\n",
    "    return torch.LongTensor([orignal_text+trans_text]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(sentence1,sentence2):\n",
    "    text = generate_text(sentence1,sentence2)\n",
    "#     tokens = tokenizer.tokenize(comment)\n",
    "#     sentence = comment\n",
    "#     if len(tokens)>400:\n",
    "#         sentence = comment[0:400]\n",
    "#     text = [tokenizer.encode(sentence, add_special_tokens=True)]\n",
    "#     text = torch.LongTensor(text)\n",
    "    mask = (text != 0).float()\n",
    "    pred = []\n",
    "    with torch.no_grad():\n",
    "        _,outputs = model(text, attention_mask=mask)\n",
    "        pred = outputs.cpu().numpy().tolist()\n",
    "#     pred = np.argmax(np.array(pred).reshape((-1,features_size,4)),axis=2)\n",
    "    return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6785768270492554, 0.6522148847579956]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_a = '技术侦查措施只能在立案后采取'\n",
    "text_b = '未立案不可以进行技术侦查'\n",
    "get_similarity(text_b,text_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity(text_b,text_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
